---
title: "R Notebook"
output: html_notebook
---
# Projekt Dokumentation

Aufgabe: Vorhersage der Umsätze vom 9.6.2019 bis 30.07.2019 

###Infos zu den gegebenen Daten

Warengruppen: 
* 1 = Brot
* 2 = Brötchen 
* 3 = Croissant
* 4 = Konditorei
* 5 = Kuchen
* 6 = Saisonbrot

Wetterdaten:
* Mittlerer Bewölkungsgrad am Tag (0 = min, 8 = max)
* MIttlere Temperatur in C
* Mittlere Windgeschwindigkeit in m/s
* Wettercode (http://www.seewetter-kiel.de/seewetter/daten_symbole.htm)
* und in der Datei wettercodes.Rda

## Vorbereitung ##

##### benötigte Libraries laden #####

```{r}

# Create list with needed libraries
pkgs <- c("lubridate", "stringr","tidyverse", "readr", 
          "fastDummies", "reticulate", "ggplot2", "Metrics")

# Load each listed library and check if it is installed and install if necessary
for (pkg in pkgs) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

```


##### Vorbereitete Datensätze laden #####

* Wetterdaten wurden in "Datenaufbereitung_Wetter.Rmd" vorbereitet
* Feiertagedaten wurden in "Datenaufbereitung_Feiertage.R" vorbereitet
* Schulferien wurden in "Datenaufbereitung_Schulferien.R" vorbereitet
...

```{r}
# Lade Daten
load("pj_wetter_dummy.Rda")
pj_wetter <- pj_wetter_dummy
  
load("kiwoDT.Rda")
pj_kiwo <- kiwoDT
  
load("umsatzDT.Rda")
pj_umsatz <- umsatzDT

# load("feiertageSH.Rda")
# pj_feiertage <- feiertageSH

load("schulferien.Rda")
pj_schulferien <- schulferien

#VPI müsste noch vorbereitet werden zur Verwendung
#pj_VPI <- read.csv(file = "verbraucherpreisindex.csv", sep= ";")

# hier gegenenenfalls noch mehr einfügen: 

# Schulferien (eventuell händisch machen) 
# verbaucherpreisindex oder Inflationsrate
# Tourismus/Kreuzfahrtdaten 
# Weizenpreis/Tag 
# Grippewellen
# Tag des Monats

# Erste Betrachtung der Daten
# hist(pj_wetter$Wettercode) fehler weil $Wettercode nicht mehr vorhanden
summary(pj_wetter)
summary(pj_kiwo)
summary(pj_umsatz)
# summary(pj_feiertage) fehler weil pj_feiertage nicht benutzt

```


## Zusammenführung und Bereinigung der Datensätze ##

```{r}

# --- Umsatz-DF optimieren und Daten zusammenstellen ---

# Aus Long-Format ein Wide-Format machen
pj_umsatz_wide <- spread(pj_umsatz, Warengruppe, Umsatz)
summary(pj_umsatz_wide)

# Wochentag hinzufügen (in neuer Spalte von pj_umsatz)
pj_umsatz_wide$Wochentag <- weekdays(pj_umsatz_wide$Datum)

# Merge erstellt automatisch die Schnittmenge
pj_umsatz_wetter <- merge(pj_umsatz_wide, pj_wetter, by="Datum")

pj_umsatz_wetter_ferien <- merge(pj_umsatz_wetter, pj_schulferien, by="Datum", all.x = TRUE)

# Der Zusatz all.x = TRUE sorgt dafür, dass keine Zeilen weggelöscht werden
allData <- merge(pj_umsatz_wetter_ferien, pj_kiwo, by="Datum", all.x = TRUE)



# Jetzt alle weiteren Datensätze hinzufügen, z.B. Feiertage etc.
# --> Feiertage sind komplett nicht mit drin, die Filiale scheint da also geschlossen gewesen zu sein
# allData <- merge(pj_um_we_ki, pj_feiertage, by="Datum", all.x = TRUE)
# Verbaucherpreisindex eventuell noch mit dazunehmen --> erstmal nicht
# allData <- merge(pj_VPI , allData)

# Datum auseinanderziehen? --> noch machen um Einfluss von Tag des Monats zu betrachten

# Warengruppen bennen und 
# NA mit 0 füllen, dort wo es Sinn ergibt (Kiwo, Saisonbrot, Schulferien)                            
 allData <- allData %>%
  rename("Brot" = `1`,
         "Brötchen" = `2`,
         "Croissant" = `3`,
         "Konditorei" = `4`, 
         "Kuchen" = `5`,
         "Saisonbrot" = `6`) %>%
    mutate_at(vars(KielerWoche, Saisonbrot, Schulferien), ~replace(., is.na(.), 0) )
 # NA zu 0 geht auch so:
# allData$Saisonbrot <- replace_na(allData$Saisonbrot, 0)
# allData$KielerWoche <- replace_na(allData$KielerWoche, 0)

# dummy coding der Wochentage
allData_dummy <- dummy_cols(allData, select_columns = "Wochentag")

# Wochentag Spalte raus 
allData_dummy$Wochentag <- NULL

# alle restlichen NAim gesamten Datensatz löschen
allData_dummy <- na.omit(allData_dummy)

# berechne den Gesamt umsatz an einem Tag und packe diesen in eine neue Spalte
allData_dummy$umsatzGes <- rowSums(allData_dummy[, c(2, 3, 4, 5, 6, 7)]) 

# Datensatz Überprüfen
summary(allData_dummy)

save(allData_dummy, file="projectData_dummy.Rda")


#------------------------------------------------------------------------------#
# AB HIER STARTEN MIT:
load("projectData_dummy.Rda")

```



## lineare Regression ##

### Maximierung des adjustierten R2

```{r}
#mod1 <- lm(`1` ~ Temperatur + as.factor(Windstaerke), allData)
#mod2 <- lm(`1` ~ Temperatur + as.factor(Windstaerke)+ as.factor(Bewoelkungsgrad), allData)
#mod3 <- lm(`1` ~ Temperatur + as.factor(Windstaerke)+ as.factor(KielerWoche), allData)
#mod4 <- lm(`1` ~ Temperatur + as.factor(Windstaerke)+ `2`, allData)
#mod4 <- lm(`2` ~ Temperatur + as.factor(Wochentag)+ `2`, allData)

#summary(mod4)
```


## Neuronales Netz ##

### Datenaufbereitung: 
1. Features und Label definieren
2. Datensatz teilen in Trainingsdaten und Validierungsdaten

!! AB HIER MÜSSEN UNSERE DATEN EINGESETZT WERDEN !!

```{r}
###################################################
### Selection of the Feature Variables and the Label Variable ####

# Selection of the features (the independent variables used to predict the dependent)
#features <- c('sqft_lot', 'waterfront', 'grade', 'bathrooms', view_dummies, condition_dummies)

bewoelkung_dummies <- c("Bewoelkungsgrad_keine", "Bewoelkungsgrad_gering", "Bewoelkungsgrad_mittel", "Bewoelkungsgrad_stark")
wettercode_dummies <- c( "Wettercode_neu_Bewölkung_auflösend_abnehmend", "Wettercode_neu_Bewölkung_nicht_beobachtet", "Wettercode_neu_Dunst_Staub", "Wettercode_neu_Ereignisse_letzte_Stunde", "Wettercode_neu_Gewitter", "Wettercode_neu_Gleichbleibende_Bewölkung", "Wettercode_neu_Nebel_Eisnebel", "Wettercode_neu_Regen", "Wettercode_neu_Schauer", "Wettercode_neu_Schnee", "Wettercode_neu_Sprühregen",         "Wettercode_neu_Trockenereignisse", "Wettercode_neu_Zunehmende_Bewölkung", "Wettercode_neu_NA")
wochentag_dummies <- c("Wochentag_Monday", "Wochentag_Tuesday", "Wochentag_Wednesday", "Wochentag_Thursday", "Wochentag_Friday", "Wochentag_Saturday", "Wochentag_Sunday")

features <- c(bewoelkung_dummies, wettercode_dummies, wochentag_dummies, "Temperatur", "Windstaerke", "KielerWoche", "Schulferien","Brot", "Brötchen", "Croissant", "Konditorei", "Kuchen", "Saisonbrot")
# Selection of the label (the dependent variable)
labels <- c("umsatzGes")


###################################################
### Selection of Training, Validation and Test Data ####

# Look at the data
str(allData_dummy)

# Setting the random counter to a fixed value, so the random initialization stays the same (the random split is always the same)
set.seed(1)

# Assign each row number in the full dataset randomly to one of the two groups (i.e. either training or validation dataset)
# Thereby the samples are randomly shuffled and 
# the probability of being in one of the groups results in corresponding group sizes
assignment <- sample(1:3, size = nrow(allData_dummy), prob = c(.7, .2, .1), replace = TRUE)

# Create training, validation and test data for the features and the labels
training_features <- allData_dummy[assignment == 1, features]    # subset house_pricing to training indices only
training_labels <- allData_dummy[assignment == 1, labels]    # subset house_pricing to training indices only
training_labels <- as.data.frame(training_labels)

validation_features <- allData_dummy[assignment == 2, features]  # subset house_pricing to validation indices only
validation_labels <- allData_dummy[assignment == 2, labels]  # subset house_pricing to validation indices only
validation_labels <- as.data.frame(validation_labels)

testing_features <- allData_dummy[assignment == 3, features]  # subset house_pricing to validation indices only
testing_labels <- allData_dummy[assignment == 3, labels]  # subset house_pricing to validation indices only
testing_labels <- as.data.frame(testing_labels)


#are there any missing values?
table(is.na(training_features))

#neuronal networks can't handle missing values --> delete!

```


### Modell aufstellen in Python

```{python}
# Import needed Python libraries and functions
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam


# The argument "input_shape" for the definition of the input layer must include 
# the number of input variables (features) used for the model. 
# To automatically calculate this number we use the function `r.training_features.keys()`, 
# which returns the list of variable names of the dataframe `training_features`.
# Then, the funtion `len()` returns the length of this list of variable names 
# (i.e. the number of variables in the input)

model = Sequential([
  InputLayer(input_shape = (len(r.training_features.keys()), )),
  BatchNormalization(),
  Dense(len(r.training_features.keys()), activation = 'relu'),
  Dense(len(r.training_features.keys())//2, activation = 'relu'),
  Dense(len(r.training_features.keys())//4, activation = 'relu'),
  Dense(len(r.training_features.keys())//8, activation = 'relu'),
  Dense(1)
])

# Ausgabe einer ZUsammenfassung zur Form des MOdells, das geschätzt wird (nicht notwendig)
model.summary()

```


### Schätzung de neuronalen Netzes

```{python}
# definition of the loss function and the optimazation function with hyperparameters
model.compile(loss="mse", optimizer=Adam(learning_rate=0.001))

#Schätzung des Modells
history = model.fit(r.training_features, r.training_labels, epochs = 150,
                    validation_data = (r.validation_features, r.validation_labels), verbose = 0)


#save model
model.save("python_model.h5")

```


### graphische Ausgabe der Modelloptimierung 

```{r}
# Graphische Ausgabe der Modelloptimierung

#create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                   loss = unlist(py$history$history$loss))

ggplot(data[-(1:10), ])+
  geom_line(aes(x = 1:length(val_loss), y = val_loss, colour = "Validation Loss")) +
  geom_line(aes(x = 1:length(loss), y = loss, colour = "Training Loss")) +
  scale_colour_manual(values = c("Training Loss"="blue", "Validation Loss" = "red")) +
  labs(title = "Loss Function Values During Optimazation") +
  xlab("Iteration Number") +
  ylab("Loss")


```

### Auswertung der Schätzergebnisse
```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
training_predictions <- py$model$predict(training_features)
validation_predictions <- py$model$predict(validation_features)
testing_predictions <- py$model$predict(testing_features)

# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(training_labels[[1]], training_predictions)*100, digits=3, nsmall=2)))

cat(paste0("\nMAPE on the Validation Data:\t", format(mape(validation_labels[[1]], validation_predictions)*100, digits=3, nsmall=2)))

cat(paste0("\nMAPE on the Testing Data:\t", format(mape(testing_labels[[1]], testing_predictions)*100, digits=3, nsmall=2)))

```

```{r}
## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten
# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = training_predictions, actual = training_labels[[1]])
data_test <- data.frame(prediction = validation_predictions, actual = validation_labels[[1]])
data_test2 <- data.frame(prediction = testing_predictions, actual = testing_labels[[1]])

# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Price in EUR") 

# Plot der Ergebnisse der Validierungsdaten
ggplot(data_test[,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Validation Data") +
  xlab("Case Number") +
  ylab("Price in EUR")

# Plot der Ergebnisse der Testdaten
ggplot(data_test2[,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Price in EUR") 
```

### Ich vermute nochmal eine deutliche Verbesserung des MAPE (und damit des NN), wenn wir die Datensätzte oder das NN so verändern das nicht diese riesigen spitzten entstehen, ### welche den Gesamtumsatzt extrem überschätzen. 

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorhergesagter Umsatz:\t", round(validation_predictions[100])))

cat(paste0("\nTatsächlicher (Validation) Umsatz:\t", validation_labels[[1]][100]))

```

